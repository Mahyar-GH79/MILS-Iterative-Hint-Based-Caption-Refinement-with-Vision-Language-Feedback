# MILS\+: Iterative Hint Based Caption Refinement with Vision Language Feedback

This repository contains the code for the report *MILS+: Iterative Hint Based Caption Refinement with Vision Language Feedback*.  
The report was prepared for course **CMSC848K** at the **Department of Computer Science, University of Maryland, College Park**.

## Overview

MILS+ is an iterative image caption refinement framework. Given an initial caption for an image (coming from the hints of a VLM), the system repeatedly improves the caption by using feedback (semantic hints) generated by a vision language model. Each refinement step incorporates this feedback to produce captions that are progressively more accurate, detailed, and faithful to the image content.

This repository provides a complete pipeline for:
- generating initial caption (Unlike the original paper, we only generate one caption)
- iteratively refining captions using hint based vision language feedback
- evaluating captions on the COCO evaluation dataset
- visualizing qualitative improvements

---

## Dataset Setup (Required)

This project uses the **COCO 2017 validation dataset**.

### Step 1: Download COCO images

Download **val2017 images** from the official COCO website, and put them under the directory data/
https://cocodataset.org/#download

Extract and place the images under:
data/val2017

Extract and place the captions under:
data/captions_val2017.json

## Environment Setup

From the project root, create and activate a virtual environment.

On Linux or macOS:
```bash
python3 -m venv env
source env/bin/activate





