# MILS\+: Iterative Hint Based Caption Refinement with Vision Language Feedback

This repository contains the code for the report *MILS+: Iterative Hint Based Caption Refinement with Vision Language Feedback*.  
The report was prepared for course **CMSC848K** at the **Department of Computer Science, University of Maryland, College Park**.

## Overview

MILS+ is an iterative image caption refinement framework. Given an initial caption for an image (coming from the hints of a VLM), the system repeatedly improves the caption by using feedback (semantic hints) generated by a vision language model. Each refinement step incorporates this feedback to produce captions that are progressively more accurate, detailed, and faithful to the image content.

This repository provides a complete pipeline for:
- generating initial caption (Unlike the original paper, we only generate one caption)
- iteratively refining captions using hint based vision language feedback
- evaluating captions on the COCO evaluation dataset
- visualizing qualitative improvements

---

## Dataset Setup (Required)

This project uses the **COCO 2017 validation dataset**.

### Step 1: Download COCO images

Download **val2017 images** from the official COCO website, and put them under the directory data/
https://cocodataset.org/#download

Extract and place the images under:
data/val2017

Extract and place the captions under:
data/captions_val2017.json

## Environment Setup

From the project root, create and activate a virtual environment.

On Linux or macOS:
```bash
python3 -m venv env
source env/bin/activate
```

Install the requirements by executing the following command:
```bash
pip install requirements.txt
```
## Running the Pipeline
Example command to run the batch caption refinement:

```bash
python run_batches.py \
  --coco_images_dir data/coco/images/val2017 \
  --ann_file data/coco/annotations/captions_val2017.json \
  --results_dir outputs/hint_only_results_qwenllm \
  --max_images 1000 \
  --num_steps 10
```

Evaluate captions using standard captioning metrics, then visualize them in plots using:
```bash
python plot_and_dataset_eval.py \
  -r outputs/hint_only_results_qwenllm \
  -a data/coco/annotations/captions_val2017.json \
  -n 10 \
  -j /usr/bin/java \
  --spice_timeout_sec 180 \
  --meteor_timeout_sec 60 \
  --image_timeout_sec 240
```
This script evaluates the refined captions using standard captioning metrics and generates plots summarizing the results. Java is required for SPICE evaluation.

## Evaluating BLEU4 (Optional)
run the following command in the project directory to add **BLEU4** to the already computed metrics:
```bash
python add_bleu4_to_metrics.py \
  --traces_dir hint_only_results_qwenllm/per_image_traces \
  --metrics_dir hint_only_results_qwenllm/per_image_metrics \
  --ann_file /home/mahyar/UMD_FinalProject/data/captions_val2017.json \
  --num_steps 10
```

